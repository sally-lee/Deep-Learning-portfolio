{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVIE2XgFxGr2"
   },
   "source": [
    "# Regularization (정규화)\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRC2-VwzxNId"
   },
   "source": [
    "높은 분산(High variance)으로 신경망이 데이터를 과적합(overfitting)하는 <br>\n",
    "문제가 의심된다면 가장 먼저 시도할것은 Regularization(정규화)이다. <br>\n",
    "높은 분산을 해결하기위해 더 많은 훈련 데이터를 얻는 방법도있지만,<br>\n",
    "이는 비용이 많이 들어가기 때문에 분산을 줄이는데 정규화를 시도할수있다. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 1. Basic idea in Logistic regression\n",
    "\n",
    "Logistic regression에서 정규화가 어떻게 작동하는지 살펴볼수있다. <br>\n",
    "\n",
    "<br>\n",
    "min$_{w,b}$ $J(w,b)$ <br>\n",
    "\n",
    "Logistic regression에서는 위와같이 정의된 비용함수 $J$를 최소화한다. <br><br>\n",
    "\n",
    "$J(w,b) = \\frac{1}{m} \\sum^{m}_{i=1}L(\\hat{y}^{(i)},y^{(i)})$ <br>\n",
    "\n",
    "개별적인 훈련샘플의 손실 예측의 합 비용함수 $J$, <br><br>\n",
    "\n",
    "$w\\in\\mathbb{R}^{n_x}$　　 $b\\in\\mathbb{R}$ <br>\n",
    "$w$는 $n_x$차원의 파라미터 벡터 , $b$는 실수이다.<br>\n",
    "<br>\n",
    "\n",
    "$J(w,b) = \\frac{1}{m} \\sum^{m}_{i=1}L(\\hat{y}^{(i)},y{(i)})$ <font color=royalblue>+ $\\frac{\\lambda}{2m} \\Vert w \\Vert ^2 _2$ </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "여기에 정규화 파라미터 $\\lambda$(람다)를 추가해준다. <br>\n",
    "$\\lambda$를 $2m$으로 나누고 $w$제곱의 노름(norm)을 곱해준다. <br><br>\n",
    "\n",
    "$\\Vert w \\Vert ^2 _2 = \\sum^{n_x}_{j=1} w_j^2 = w^Tw$\n",
    "\n",
    "여기서 $w$제곱의 노름은 $j$의 1부터 $n_x$까지 $w_j^2$의 값을 더한것과 같다. <br>\n",
    "$w$의 전치행렬($w^T$) 곱하기 $w$와도 같다. <br>\n",
    "이것을 $L2$ regularization 이라고 부른다. <br>\n",
    " $L2$ 노름을 사용해서 $\\Vert w \\Vert^2_2$ 라고 표현해준다. <br>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "* 왜 $w$만 정규화하고 $b$에 관해서는 추가하지않을까? <br>\n",
    "\n",
    "실제로는 가능하지만 보통 생략한다. <br>\n",
    "높은 분산을 가질때 $w$는 많은 파라미터를 갖지만 b는 하나의 숫자이기때문이다. <br> 따라서 거의 모든 파라미터는 $b$가 아닌 $w$에 있다. <br>\n",
    "$b$에 lambda 항을 추가해줘도 되지만 실질적인 차이는 크게 없다. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "* $L1$ 정규화란? <br>\n",
    "\n",
    "$L2$ 노름 대신에 다음과 같은 항을 추가한다. <br><br>\n",
    "$\\frac{\\lambda}{2m}\\sum^{n_x}_{j=1}\\vert w_j \\vert$ = $\\frac{\\lambda}{2m} \\Vert w \\Vert _1$ <br>\n",
    "파라미터 $w$벡터의 $L1$노름이라고도 한다. <br>\n",
    "아래첨자 1을 확인할수 있다. $m$앞의 2는 스케일링 상수이다.<br>\n",
    "$L1$ 정규화를 사용하게되면 $w$가 희소해지는데 이는 $w$벡터안에 0이 많아진다는 의미이다. <br>\n",
    "하지만 모델을 압축하겠다는 목표가 있지않는 이상 $L1$ 정규화는 잘 사용하지 않는다. <br><br>\n",
    "\n",
    "\n",
    "* $\\frac{\\lambda}{2m}$ 에서 $\\lambda$를 정규화 파라미터라고 부른다.<br>\n",
    "\n",
    "개발 세트(Dev set) 혹은 교차검증(Cross validation) 세트를 주로 사용한다. <br>\n",
    "다양한 값을 시도해서 훈련 세트에 잘 맞으면서 두 파라미터의 노름을 잘 설정해 <br> 과적합(overfitting)을 막을 수 있는 최적의 값을 찾는다. <br><br>\n",
    "\n",
    "따라서 $\\lambda$는 설정이 필요한 또 다른 하이퍼 파라미터이고, <br>\n",
    "이것이 Logistic regression에 대한 $L2$ Regularization 를 나타내는 방법이다. <br>\n",
    "<br>\n",
    "\n",
    "> 파이썬에서 $\\lambda$를 쓸때 <font color=royalblue>lambda</font> 내장함수를 주의하자\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuFECkZeRnTi"
   },
   "source": [
    "## 2. L2 Regularization in Neural network\n",
    "\n",
    "<br>\n",
    "\n",
    "$J(w^{[1]},b^{[1]}, ... , w^{[l]},b^{[l]}) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{(i)}y^{(i)})$ <br>\n",
    "\n",
    "신경망에는 $w^{[1]},b^{[1]}$부터 $w^{[l]},b^{[l]}$까지의 모든 파라미터를 갖는 비용 함수가 있다. <br> \n",
    "$l$은 신경망 층의 개수이고 비용함수는 훈련샘플 $m$까지의 손실의 합을 $m$으로 나눈 값이다. <br><br>\n",
    "\n",
    "$J(w^{[1]},b^{[1]}, ... , w^{[l]},b^{[l]}) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{(i)}y^{(i)})$ + <font color=royalblue> $\\frac{\\lambda}{2m} \\sum^{L} _{l=1} \\Vert w^{[l]} \\Vert ^2 $ </font> <br>\n",
    "\n",
    "여기에 정규화를 더하기위해 $\\lambda$를 $2m$으로 나눈 값$\\times$파라미터 $w$ 노름 제곱의 모든 값을 더해준다. <br> <br>\n",
    "\n",
    "$\\Vert w^{[l]} \\Vert ^2 = \\sum^{n^{[l-1]}}_{i=1} \\sum^{n^{[l]}}_{j=1} (w^{[l]}_{ij})^2$ <br>\n",
    "\n",
    "여기있는 행렬의 노름의 제곱은 $i$와 $j$에 해당하는<br>\n",
    "각각의 행렬의 원소를 제곱한 것을 모두 더해준 값이다. <br>\n",
    "합의 범위는 $i$는 1부터 $n^{[l-1]}$이고, $j$는 1부터 $n^{[l]}$까지로 한다. <br> 왜냐하면 $w$는 $(n^{[l-1]},n^{[l]})$ 차원의 행렬이기 때문이다. <br>\n",
    "해당 층 $l-1$과 $l$의 은닉 유닛의 개수를 나타낸다. <br><br>\n",
    "\n",
    "$\\Vert w^{[l]} \\Vert ^2 _F$ <br>\n",
    "\n",
    "이 행렬의 노름은 $L2$ 노름이라고 부르는 대신 <br>\n",
    "프로베니우스 노름이라고 부르고 아래첨자 $F$를 붙여준다. <br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Gradient descent 구하기 <br><br>\n",
    "\n",
    "$dw^{[l]} =$ (from backward prop) <br>\n",
    "$\\frac{dJ}{dw^{[l]}} = dw^{[l]}$ <br>\n",
    "\n",
    "backward propagation에서 $dw$를 계산했을때 <br>\n",
    "$w$에 대응하는 $J$의 편미분 값을 제공했다. <br>\n",
    "주어진 $l$에 대한 $w$이다. <br><br>\n",
    "\n",
    "$w^{[l]} := w^{[l]} - \\alpha dw^{[l]}$ <br>\n",
    "그리고 $w^{[l]}$에 $-$학습률$\\times dw^{[l]}$로 업데이트해주었다. <br>\n",
    "이것은 정규화 항을 더해주기 이전의 값이다. <br><br>\n",
    "\n",
    "\n",
    "$dw^{[l]} =$ (from backward prop)<font color=purple> $+\\frac{\\lambda}{m}w^{[l]}$ </font> <br>\n",
    "여기에 $\\lambda$ 정규화 항을 더해주게된다. <br>\n",
    "그리고 다시 $w^{[l]}$를 업데이트해준다. <br>\n",
    "<font color=purple> -> </font> $w^{[l]} := w^{[l]} - \\alpha dw^{[l]}$ <br><br>\n",
    "\n",
    "\n",
    "$(\\frac{\\lambda}{2m} \\Vert w^{[l]} \\Vert ^2 )' = \\frac{\\lambda}{m}w^{[l]}$ <br>\n",
    "\n",
    "이것은 여전히 비용함수의 미분에 대한 올바른 정의이다. <br>\n",
    "파라미터에 관해 끝에 정규화 항을 더해준것 뿐이다. <br><br>\n",
    "\n",
    "\n",
    "<font color=green> $dw^{[l]}$</font> $=$ (from backward prop) $+\\frac{\\lambda}{m}w^{[l]}$ <br>\n",
    "여기 $dw^{[l]}$의 정의를 아래에 적용시키면, <br>\n",
    "$w^{[l]} := w^{[l]} - \\alpha$<font color=green> $dw^{[l]}$ </font> <br><br>\n",
    "\n",
    "\n",
    "$w^{[l]} := w^{[l]} - \\alpha$<font color=green>$[$(from back prop)$ + \\frac{\\lambda}{m}w^{[l]}]$ </font><br>\n",
    "　　　$=w^{[l]} - \\frac{\\alpha\\lambda}{m}w^{[l]} - \\alpha$(from backprop) <br>\n",
    "\n",
    "\n",
    "　　　$=w^{[l]}(1- \\frac{\\alpha\\lambda}{m})$ $- \\alpha$(from backprop)<br><br>\n",
    "따라서 행렬 $w^{[l]}$이 어떤값이든 값이 약간 더 작아진다는것을 알수있다. <br>\n",
    "\n",
    "이러한 이유로 $L2$ 정규화를 가중치 감쇠(Weight decay)라고 부르기도 한다. <br>\n",
    "1보다 살짝 작은 값을 가중치 $w$행렬에 곱해준다는 이유에서 나온 이름이다. <br>\n",
    "<br>\n",
    "\n",
    "이것이 신경망에서 $L2$ 정규화를 구현하는 방법이다. <br>\n",
    "이 다음엔 정규화가 어떻게 과적합을 막는지에 대해 알아볼 수 있다. <br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "참조 : https://youtu.be/6g0t3Phly2M \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkRpAtcPomQ4Z9M0uQK9gP",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
