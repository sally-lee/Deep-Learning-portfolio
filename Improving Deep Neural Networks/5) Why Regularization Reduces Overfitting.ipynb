{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AW3Si81CiyXq"
   },
   "source": [
    "# Why Regularization Reduces Overfitting\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "왜 Regularization(정규화)이 과적합(overfitting)문제를 해결하고 <br>\n",
    "분산을 줄이는데 도움이 될까? <br>\n",
    "이것이 어떻게 작동하는지에대해 몇가지 예제를 살펴볼수있다. <br>\n",
    "<br>\n",
    "\n",
    "\n",
    "## How does regularization prevent overfitting?\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1. neural network intuition\n",
    "<br>\n",
    "<br>\n",
    "<img src=/image/01.png> <br>\n",
    "\n",
    "높은 편향(high bias), 높은 분산(high variance), <br>\n",
    "그리고 적절한 경우(just right)의 예시가 있다. <br>\n",
    "이것을 크고 깊은 신경망에 맞추는 경우를 살펴보자. <br><br>\n",
    "\n",
    "\n",
    "$J(w^{[l]},b^{[l]}) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{(i)},y^{(i)})$ <br>\n",
    "\n",
    "신경망에 과적합의 문제가 있다고 할때 $w$와 $b$에 대한 비용함수 $J$는 <br>\n",
    "1부터 $m$까지 손실의 합과 같다. <br><br> \n",
    "\n",
    "$J(w^{[l]},b^{[l]}) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{(i)},y^{(i)})$ + <font color=purple> $\\frac{\\lambda}{2m} \\sum^{L} _{l=1} \\Vert w^{[l]} \\Vert ^2 _F$ </font> <br>\n",
    "\n",
    "정규화를 위해 추가적인 정규화 항을 더해줄 수 있다. <br>\n",
    "가중치 행렬이 너무 커지지 않도록 막기 위해서이다. <br><br>\n",
    "\n",
    "그렇다면 프로베니우스 노름을 줄이는것이 왜 과적합을 줄일 수 있을까? <br>\n",
    "여기서 알수있는 것은 정규화에서 $\\lambda$를 크게 만들어서 <br>\n",
    "가중치 행렬 $w$를 0에 상당히 가깝게 설정할 수 있다는 것이다. <br>\n",
    "\n",
    "<img src=/image/02.png> <br>\n",
    "\n",
    "따라서 많은 은닉 유닛을 0에 가까운 값으로 설정해서 <br>\n",
    "은닉 유닛의 영향력을 줄이게 된다. <br>\n",
    "그런 경우에 훨씬 더 간단하고 작은 신경망이 될것이다. <br>\n",
    "다양한 층을 갖고있긴 하지만 Logistic regression에 가까워진다. <br><br>\n",
    "\n",
    "\n",
    "<img src=/image/03.png> <br>\n",
    "\n",
    "따라서 이런 과적합한 경우를 왼쪽의 높은 편향(high bias)의 <br>\n",
    "경우와 가깝게 만들어 줄 수 있다. <br>\n",
    "중간의 딱 맞는 경우과 가깝게하는 적절한 $\\lambda$값을 찾는것이 가장 좋을것이다. <br><br>\n",
    "\n",
    "어떤 뭉치의 은닉 유닛을 완전히 0으로 만드는것은 아니다.<br>\n",
    "핵심은 $\\lambda$의 값을 아주 크게하면 $w$는 0에 가깝게 설정된다는 것이다. <br>\n",
    "즉 은닉 유닛의 영향력을 0에 가깝게 줄임으로써 <br>\n",
    "Logistic regression과 가까운 네트워크를 만든다고볼수있다. <br>\n",
    "간단한 네트워크는 과적합의 문제가 덜 일어나게되니 <br>\n",
    "어떻게 정규화가 과적합을 해결할수있는지 이해할 수 있다. <br><br>\n",
    "\n",
    "\n",
    "### 2. tanh activation function intuition\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=/image/04.png> <br>\n",
    "\n",
    "왜 정규화가 과적합을 막는데 도움을 주는지 또다른 직관적인 예시가 있다. <br>\n",
    "tanh 활성화 함수를 사용한다고 가정해보자. <br>\n",
    "$g(z) = $ tanh$(z)$ 인 경우이다. <br> \n",
    "이 경우 $z$가 아주 작은 경우에, <br>\n",
    "$z$가 작은 범위의 파라미터를 갖는 경우엔 tanh함수의 선형 영역을 사용하게된다. <br> $z$값이 더 작아지거나 커지면 활성화 함수는 선형 영역을 벗어난다. <br><br>\n",
    "\n",
    "$\\lambda \\uparrow$ 　　  $w\\downarrow$ <br>\n",
    "즉, 정규화 파라미터 $\\lambda$가 커질때 <br>\n",
    "비용함수가 커지지 않으려면 상대적으로 $w$가 작아질 것이다. <br><br>\n",
    "\n",
    "$ z^{[l]} : w^{[l]} a^{[l-1]} + b^{[l]} $ <br>\n",
    "가중치 $w$가 작으면 $z$도 상대적으로 작은 값을 가지게 된다. <br>\n",
    "$z$가 작은 값을 갖게되면 $g(z)$는 선형 영역만을 가지는 거의 1차원 함수가 된다. <br>따라서 모든 층은 linear regression(선형 회귀)처럼 거의 직선의 함수를 갖게 된다. <br>\n",
    "모든 층이 선형이면 전체 네트워크도 선형이다. <br>\n",
    "그래서 깊은 네트워크라도 선형 활성화 함수를 가졌다면 선형 함수만을 계산하게된다. <br><br>\n",
    "\n",
    "<img src=/image/05.png> <br>\n",
    "\n",
    "\n",
    "그러므로 매우 복잡한 결정인 비선형 결정경계(decision boundary)에 맞추기는 불가능하다. <br>\n",
    "high variance(높은 분산)의 경우처럼 과적합된 데이트세트까지 맞추기는 어렵다. <br>\n",
    "정리하자면 정규화 파라미터 $\\lambda$가 매우 크면 파라미터 $w$는 작아지고 $z$의 값도 작아진다. <br>\n",
    "$z$가 작은 범위의 값을 가지기 때문에 tanh의 경우 활성화 함수는 상대적으로 선형이 된다. <br>\n",
    "전체 신경망은 선형 함수로부터 그리 멀지않은곳에서 계산되어 <br>\n",
    "매우 복잡한 비선형 함수보다 훨씬 간단한 함수가 될 것이다. <br>\n",
    "따라서 과적합의 가능성이 줄어든다. <br><br>\n",
    "\n",
    "\n",
    "### 3. regularization 구현시 주의사항\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$J(w^{[l]},b^{[l]}) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{(i)}y^{(i)})$ + <font color=purple> $\\frac{\\lambda}{2m} \\sum^{L} _{l=1} \\Vert w^{[l]} \\Vert ^2 _F$ </font> <br>\n",
    "\n",
    "정규화를 구현할때 비용함수 J를 살펴보았다. <br>\n",
    "여기에 가중치가 너무 커지는것을 막기위해 추가적인 항을 추가하였다. <br>\n",
    "경사 하강법(gradient descent)를 정의하는 한 단계는 <br>\n",
    "경사 하강법의 반복의 수에 대한 함수로 비용함수를 설정하는것이다. <br><br>\n",
    "\n",
    "<img src=/image/006.png> <br>\n",
    "\n",
    "비용함수 $J$가 경사 하강법의 반복마다 단조감소하기를 원할 것이다. <br>\n",
    "그러나 정규화를 구현할때 비용함수 $J$의 예전 정의, <br>\n",
    "즉 첫 항만을 그린다면 단조 감소를 보지 못할것이다. <br>\n",
    "따라서 경사 하강법을 디버깅할때는 두번째 항을 포함한 <br>\n",
    "새로운 비용함수 $J$를 그리고있는지 잘 확인해야한다. <br><br>\n",
    "이렇게 $L2$ 정규화 기법을 알아보았다. <br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "참조 : https://youtu.be/NyG-7nRpsW8"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO1QXKeyrn7iktCQ+nsTzw2",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
