{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_ryxGKyIMHm"
   },
   "source": [
    "# Other Regularization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9U-OatsLTY_"
   },
   "source": [
    "## 학습 목표\n",
    "- L2 정규화와 drop-out외 overfitting을 줄일 수 있는 다른 방법 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW0XIDt9Iv9b"
   },
   "source": [
    "## 1. Data augmentation (데이터 증가)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03RnzZ6HI106"
   },
   "source": [
    "### 배경\n",
    "- 모델을 훈련시키는 경우 더 많은 훈련 데이터가 overfitting을 해결하는데 도움을 줄 수 있지만 <br>\n",
    " 많은 비용이 들어가거나 불가능한 경우가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h90ko4MEacP8"
   },
   "source": [
    "<center><img src=\"images/data_augmentation.PNG\"height=\"600px\" width=\"600px\" /></center>\n",
    "<p align=\"center\"> Figure1. Data augmentation </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZLe9bx1JK5g"
   },
   "source": [
    "### 방법1\n",
    "- **수평방향으로 뒤집은 이미지**를 train set에 추가시켜서 train set를 두배로 증가시킨다.\n",
    "- 새로운 이미지를 더 많이 구하지않고 할 수 있는 방법\n",
    "> 단점 : 새로운 m개의 독립적인 샘플을 얻는것보다 중복된 샘플들이 많아져서 좋지 않다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7XhDCfqKIbG"
   },
   "source": [
    "### 방법2 \n",
    "- 이미지를 **회전시키고 무작위로 확대시킨다.**\n",
    "- 이미지의 무작위적인 왜곡과 변형을 통해 data set를 증가시키고 추가적인 가짜 train sample을 얻을수 있다. <br>\n",
    "추가적인 가짜 이미지들은 완전히 새로운 독립적인 고양이 샘플을 얻는 것보다 더 많은 정보를 추가해주지는 않을 것이다.\n",
    "> 장점 : 컴퓨터적인 비용을 들지 않고 할 수 있다. <br>\n",
    "> 데이터를 더 얻을 수 있는 비싸지 않은 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJGAXQ1YLy18"
   },
   "source": [
    "### 방법3\n",
    "- 시각적인 문자 인식의 경우, 무작위의 회전과 왜곡을 부여할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOhODOmbMEVI"
   },
   "source": [
    "### **data augmentation은 정규화 기법과 비슷하게 사용될수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC4X93tWMRH7"
   },
   "source": [
    "## 2. Early Stopping (조기 종료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwS3MWYB-4W0"
   },
   "source": [
    "### 정의\n",
    "- 경사 하강법을 실행하면서 train set에 대한 분류 오차, 즉 train 오차를 그리거나 최적화하는 비용함수 J를 그리는데 <br> \n",
    "train 오차나 비용함수 J는 다음과 같이 단조 감소하는 형태로 그려져야 하고 <br> \n",
    "dev 세트 오차가 내려가다가 mid-size에서 증가하는데 <br> \n",
    " **w가 mid-size값을 가질때 iteration을 종료하는것**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwieP4ldaV88"
   },
   "source": [
    "<center><img src=\"images/early_stopping.PNG\"height=\"600px\" width=\"600px\" /></center>\n",
    "<p align=\"center\"> Figure2. Early Stopping </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVAIjgkxB7rk"
   },
   "source": [
    "### 특징\n",
    "- 이 mid-size 주변에서 가장 잘 작동하는것을 알 수 있다.\n",
    "\n",
    "- 신경망에서 많은 반복을 실행시키지 않은 경우 parameter w는 0에 가깝다.<br>\n",
    "무작위의 작은 값으로 초기화시켜서 오랜 시간 훈련시키기 전까지 w의 값은 여전히 작다.\n",
    "\n",
    "- 반복을 실행할수록 w의 값은 계속 커진다.\n",
    "\n",
    "- L2 정규화와 비슷하게 parameter w에 대해 더 작은 노름을 갖는 신경망을 선택함으로써 신경망이 덜 overfitting하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlY476j9UbA6"
   },
   "source": [
    "### Early stopping 장점\n",
    "- 경사하강법 과정을 많은 값을 시도할 필요없이 한번만 실행해서 L2 정규화의 하이퍼파라미터 λ처럼 작은 w, 중간 w, 큰 w의 값을 얻게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1m5UJncDqr9"
   },
   "source": [
    "### Early stopping 단점\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zV-mRsxhCHN8"
   },
   "source": [
    "#### ML 과정-2단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyE_ZMF2CQ38"
   },
   "source": [
    "##### 1. 비용함수 J를 최적화하는 알고리즘을 원한다.\n",
    "- $J(w,b)$가 가능한 작아지는 값을 찾는것\n",
    "- Ex) 모멘텀, RMSProp, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HRJk_ALCav0"
   },
   "source": [
    "##### 2. Overfitting을 막는 방법(분산을 줄이는 방법)\n",
    "- 정규화, 데이터 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEQHPvSNYyDE"
   },
   "source": [
    "- 위 2단계를 섞어버려서 독립적으로 작업할 수 없다.<br>\n",
    "&rarr; 해결 방법 **Orthogonalization(직교화)**\n",
    "> 한번에 하나의 할일만 생각한다.\n",
    "\n",
    "- 경사 하강법을 일찍 멈춤 &rarr; 비용함수 J를 최적화하는 것을 멈춤  <br>\n",
    "&rarr;  비용함수 J를 줄이는 일을 잘 하지 못하게 되고 동시에 overfitting을 막으려고 한다. <br>\n",
    " &rarr; 따라서 두 문제를 해결하기 위해 서로 다른 도구를 사용하는 대신 혼합된 하나의 도구를 사용하게 되고 문제를 더 복잡하게 만든다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyogI3J-SHEh"
   },
   "source": [
    "### 대안: L2 정규화를 사용\n",
    "&rarr; 가능한 오래 신경망을 훈련시킬 수 있게 되고 이를 통해 하이퍼파라미터의 탐색 공간이 더 분해하기 쉽고 찾기 쉬워진다.\n",
    "- 단점 : 정규화 parameter λ에 많은 값을 시도해야 한다 &rarr; 비용이 많이 든다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s_zuqI-Uqqp"
   },
   "source": [
    "> 이런 단점에도 불구하고 많은 사람들이 early stopping 기법을 사용한다. <br>\n",
    "> L2 정규화를 사용해 λ에 많은 값을 시도하는 방법을 선호한다.(컴퓨터적으로 감당할 수 있다면)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXCdGkFzfyBF"
   },
   "source": [
    "---\n",
    "# 참고\n",
    "[Andrew Ng-Other Regularization Methods](https://www.youtube.com/watch?v=BOCLq2gpcGU&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=8)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMWXQfIgbwBXwipx/79jiYy",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
