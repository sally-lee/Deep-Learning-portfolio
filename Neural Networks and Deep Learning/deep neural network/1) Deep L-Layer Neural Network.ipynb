{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJGlIlW75mVc"
   },
   "source": [
    "# Deep L-Layer Neural Network\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ5JjVoU5sZs"
   },
   "source": [
    "그동안 Logistic regression 분석 뿐 아니라 단일 은닉층(1 hidden layer)을 <br>\n",
    "가진 신경망의 foward propagation과 backward propagation을 살펴보았다. <br>\n",
    "Vectorization과 무작위로 가중치를 초기화하는 이유도 확인했다. <br>\n",
    "이것들은 모두 심층 신경망(Deep Neural Network)를 구성하는 중요한 아이디어들이다. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkx2fHtH7LH0"
   },
   "source": [
    "## 1. What is a Deep Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4dIqMh1sgAT"
   },
   "source": [
    "<img src=\"/image/05.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB1T_ZB-ErrZ"
   },
   "source": [
    "위의 ①,②는 각각 Logistic regression과 하나의 은닉층을 갖는 신경망이다. <br>\n",
    "아래 ③,④에는 2개의 은닉층을 갖는 신경망과 5개의 은닉층을 갖는 신경망의 예시가 있다. <br>\n",
    "일반적으로 ①번 logistic regression은 얕은 모델(Shallow model)이라고 한다. <br>\n",
    "반면 ④번(5개의 은닉층을 갖는 신경망)은 깊은 모델(Deep model)이다. <br>\n",
    "얕고 깊음은 상대적인 정도의 문제이다. <br>\n",
    "②번 모델은 ④번 모델보다는 얕지만 ①번보다는 깊은 모델이다. <br>\n",
    "②번 모델은 두개의 층(2 layers)이 있는 신경망이다. <br>\n",
    "신경망의 층을 셀 때에 입력층은 세지 않기때문에 은닉층과 출력층만 세게 된다. <br>\n",
    "그러므로 ①번 logistic regression은 한 층의 신경망이다. <br> <br>\n",
    "\n",
    "지난 몇년간, AI 머신러닝 커뮤니티는 보통의 얕은 모델은 할수없는, <br>\n",
    "심층 신경망으로 학습할수있는 함수들이 있다는것을 깨달았다. <br>\n",
    "물론 주어진 어떠한 문제에 대해서 <br>\n",
    "얼마나 깊은 신경망을 사용해야하는지 미리 정확하게 예측하기란 어렵다. <br>\n",
    "따라서 Logistic regression을 시도하고 그 다음에 은닉층을 추가해서 차례차례 시도해본다. <br> \n",
    "은닉층의 개수가 또 다른 하이퍼 파라미터(파라미터를 컨트롤하는 파라미터)가 되고 <br>\n",
    "개발 설정 과정에서 다양한 값들을 시도하고 검증 데이터에서 평가한다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt9HLcMoT9PR"
   },
   "source": [
    "## 2. Deep neural network notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLBvXINHUF_w"
   },
   "source": [
    "심층 신경망을 설명하기 위한 표기법을 알아보자 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iR3DwxNsmRL"
   },
   "source": [
    "<img src=\"/image/07.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6XsBbzGh-E2"
   },
   "source": [
    "위 그림엔 <font color = royalblue> 1,2,3,4</font>개 층의 신경망이 있다. <br>\n",
    "은닉층(hidden layer)은 3개, 각각의 은닉층 유닛 개수는 <font color = green>5,5,3</font>개 이고 <br>\n",
    "출력층(output layer)은 <font color = green>1</font>개가 있다. <br>\n",
    "대문자 $L$을 써서 네트워크 층의 수를 나타낼수있다. <br>\n",
    "이 경우엔 $L =$ <font color = royalblue>$4$</font> (층의 개수)<br>\n",
    "그리고 $n$에 위첨자 $l$을 써서 소문자 $l$층의 유닛 개수를 나타낼수있다. <br>\n",
    "이제 입력 레이어를 <font color = royalblue>layer 0</font>으로 인덱스하면 차례대로 <font color = royalblue>layer 1</font>, <font color = royalblue>layer 2</font>, <font color = royalblue>layer 3</font>, <font color = royalblue>layer 4</font>가 된다. <br>\n",
    "그럼 $n^{[1]} = 5$ 가 된다. <br>\n",
    "왜냐하면 <font color = royalblue>layer 1</font>에는 <font color = green>5</font>개의 은닉층이 있기 때문이다. <br>\n",
    "두번째 은닉층의 개수도 마찬가지로 $n^{[2]} = 5$ <br>\n",
    "$n^{[3]} = 3$ <br>\n",
    "$n^{[4]} = n^{[l]} = 1$ <br>\n",
    "output layer의 유닛은 1개이다. ($L = 4$)<br>\n",
    "그리고 $x_1 , x_2 , x_3 $ 입력층(input layer)은 <br>\n",
    "$n^{[0]} = n_x = 3$ <br>\n",
    "이렇게 서로다른 층에서 노드의 개수를 나타낼수있다. <br> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuUaNaHOsp87"
   },
   "source": [
    "<img src=\"/image/09.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbFafOeAmMqo"
   },
   "source": [
    "<br>\n",
    "\n",
    "그리고 각각의 층에서 $a^{[l]}$을 사용하는데 <br>\n",
    "이것은 층 $l$에서의 활성값을 나타낸다. <br>\n",
    "뒤에서 하게될 forward propagation에서 <br>\n",
    "$a^{[l]}$는 $z^{[l]}$에 대해 활성화 함수 $g$를 계산한 값이 된다. <br>\n",
    "활성화 함수도 층$l$에 의해 순서가 매겨진다. <br>\n",
    "가중치 $w^{[l]}$와 $b^{[l]}$도 $z^{[l]}$값을 계산하기 위해 사용된다. <br>\n",
    "마지막으로 입력 특성(input feature) $x$는 layer 0의 활성값과 같다. 따라서 <br>\n",
    "<font color = purple> $x = a^{[0]}$ </font> <br>\n",
    "마지막 층의 활성값인 $a^{[l]}$은 $y$의 예측값과 같고 <br>\n",
    "이것은 신경망의 예측된 출력값 <font color = purple> $\\hat{y}$ = $a^{[l]}$ </font> 이다. <br> <br>\n",
    "\n",
    "이렇게 심층 신경망이 어떻게 생겼는지, <br>\n",
    "심층 신경망을 설명하고 계산하는데 사용되는 표기법이 어떤것인지 살펴보았다. <br>\n",
    "각각의 부호가 의미하는 바를 명확히 이해하고 표기해야 복잡한 신경망을 오류없이 다룰 수 있다. <br> <br>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "참조 : https://youtu.be/2gw5tE2ziqA\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJnEz8USjF34RqJTJz7r+I",
   "collapsed_sections": [],
   "name": "1) Deep L-Layer Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
