{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dFeQ0b90lVl"
   },
   "source": [
    "# Why Non-linear Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSBZTwCb0t80"
   },
   "source": [
    "##**1. 개요**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ6UEe5A0y18"
   },
   "source": [
    "왜 신경망은 Non-linear Activation Function이 필요할까\n",
    "\n",
    "신경망이 좀더 다양한 함수를 계산하려면 필요하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaOg9P6r04Kl"
   },
   "source": [
    "## **2. Non-linear Activation Function 필요한 이유**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW4otzXy1FfM"
   },
   "source": [
    "$z^{[1]} = W^{[1]}x + b^{[1]}$  \n",
    "\n",
    "$a^{[1]} =z^{[1]}$  \n",
    "\n",
    "\n",
    "$z^{[2]} = W^{[2]}a^{[1]} + b^{[1]}$ \n",
    "\n",
    "$a^{[2]} = z^{[2]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3IdX_1l1lWz"
   },
   "source": [
    "위의 모델은 $\\hat{y}$을 입력 feature x에 대한 Linear Function 로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuW-nsga2gD7"
   },
   "source": [
    "$a^{[1]}=z^{[1]}=W^{[1]}x+b^{[1]}$  \n",
    "$a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$  \n",
    "$a^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}$  \n",
    "$= W^{[2]}W^{[1]}x+ (W^{[2]}b^{[1]}+b^{[2]})$  \n",
    "$= W^{`}x + b^{`}$\n",
    "\n",
    "\n",
    "\n",
    "$W^{`} = W^{[2]}W^{[1]}$  \n",
    "$b^{`} = W^{[2]}b^{[1]}+b^{[2]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVtOk-2M3lN7"
   },
   "source": [
    "* Linear Activation Function 를 사용한다면\n",
    "신경망은 입력의 선형식 만을 출력한다.\n",
    "\n",
    " * 두 선형 함수의 조합은 결국 하나의 선형 함수가 된다.  \n",
    "$\\Rightarrow$ 층이 얼마나 많든 간에 신경망은 Linear Activation Function만 계산하여\n",
    "은닉층이 없는 것과 다름없다.     \n",
    "따라서 은닉층을 유의미하게 이용하기위해서는 Non-linear Activation Function 이 필요하다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzCqIy7f7e7C"
   },
   "source": [
    "* Linear Activation Function은 Regression 문제에 대한 머신러닝을 할 때 주로 사용한다.  \n",
    " * 집값을 예측할 때($y\\in \\mathbb{R}$) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_-Mz2TR8Lui"
   },
   "source": [
    "## **3.결론**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-t--wDy8OYa"
   },
   "source": [
    "* 선형 활성화 함수를 쓸 수 있는 곳은 대부분 출력층이다.\n",
    "* 은닉 유닛은 Linear Activation Function가 아닌 ReLU, tanh, leaky ReLU나 다른 Non-linear Function를 써야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKugSFTeIQBm"
   },
   "source": [
    "### 참조\n",
    "https://www.youtube.com/watch?v=NkOv_k7r6no&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm0MMxIf0iaJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8n1YYebc90TDzATtW5PAq",
   "collapsed_sections": [],
   "name": "W2L7. Why Non-linear Activation Functions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
