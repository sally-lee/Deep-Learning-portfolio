
정확도를 높이기 위해 한일

1.Avgpooling을 Maxpooling으로 바꾸자

기존 모델을 avgpooling을 썼는데 maxpooling으로 바꿔서 구동
->정확도가 epoch10까지 올라가다가 점점 과적합이 되는것을 볼 수 있었음



2.Dropout의 상수를 점점 높여주자
그래서 과적합을 막기위해 dropout을 0.2->0.4->0.8서서히 올려줌
issue)처음에는 0.1로 늘렸다가 너무 loss값이 높아져서 서서히 높이기로 결정
->loss값이 떨어지는 것을 알 수 있었고 inference해본결과 오차거리가 1.52정도가됨















3.학습률을 처음에는 높게 finetuning 시에는 낮추어서 학습하자
처음에 모델을 구동할때(epoch20까지)는 학습률을 0.016으로 비교적 높게 잡았고
그다음 부터는 학습률을 0.001로 낮추어서 finetuning
->오차거리 1.41


































4.layer층을 추가해보자
경북대 석사의 파워포인트중 layer층을 3개층에서 6개층으로 올려서 정확도를 높였다는 말이있어서
나도 layer층을 늘려줌
->모델 네트워크 구조를 아예 바꾸었기때문에 기존에  pretrained checkpoint를 쓰지못하였고 처음 부터 모델 구동
결과: loss값이 3이상되는것을 보고 아무리 하이퍼 파라미터를 바꾸어도 기존의 1.41오차보다는 줄어들지 못할거 같아서보류






























<모델 코드>
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl

from gazetrack_data import gazetrack_dataset
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau


class eye_model(nn.Module):
    def __init__(self):
        super(eye_model, self).__init__()
        
        self.model = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=0),
            nn.BatchNorm2d(32, momentum=0.9),
            nn.LeakyReLU(inplace=True),
            nn.AvgPool2d(kernel_size=2),
            nn.Dropout(0.02),
            
            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=0),
            nn.BatchNorm2d(64, momentum=0.9),
            nn.LeakyReLU(inplace=True),
            nn.AvgPool2d(kernel_size=2),
            nn.Dropout(0.02),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(128, momentum=0.9),
            nn.LeakyReLU(inplace=True),
            nn.AvgPool2d(kernel_size=2),
            nn.Dropout(0.02),
        )

    def forward(self, x):
        x = self.model(x)
        return x
    
class landmark_model(nn.Module):
    def __init__(self):
        super(landmark_model, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(8, 128),
            nn.BatchNorm1d(128, momentum=0.9),
            nn.ReLU(inplace=True),
            nn.Linear(128, 16),
            nn.BatchNorm1d(16, momentum=0.9),
            nn.ReLU(inplace=True),
            nn.Linear(16, 16),
            nn.BatchNorm1d(16, momentum=0.9),
            nn.ReLU(inplace=True),
        )
        
    def forward(self, x):
        x = self.model(x)
        return x
    
class lit_gazetrack_model(pl.LightningModule):
    def __init__(self, data_path, save_path, batch_size, logger, workers=20):
        super(lit_gazetrack_model, self).__init__()
        
        self.lr = 0.016
        self.batch_size = batch_size
        self.data_path = data_path
        self.workers = workers
        print("Data path: ", data_path)
        self.save_path = save_path
        PARAMS = {'batch_size': self.batch_size,
                  'init_lr': self.lr,
                  'data_path': self.data_path,
                  'save_path': self.save_path,
                    'scheduler': "Plateau"}
        logger.log_hyperparams(PARAMS)
        
        self.eye_model = eye_model()
        self.lmModel = landmark_model()
        self.combined_model = nn.Sequential(nn.Linear(512+512+16, 8),
                                            nn.BatchNorm1d(8, momentum=0.9),
                                            nn.Dropout(0.12),
                                            nn.ReLU(inplace = True),
                                            nn.Linear(8,
