{"cells":[{"cell_type":"markdown","metadata":{"id":"b518b04cbfe0"},"source":["##### Copyright 2020 The TensorFlow Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","execution":{"iopub.execute_input":"2021-08-25T18:01:25.218618Z","iopub.status.busy":"2021-08-25T18:01:25.217943Z","iopub.status.idle":"2021-08-25T18:01:25.220674Z","shell.execute_reply":"2021-08-25T18:01:25.221044Z"},"id":"906e07f6e562"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"a5620ee4049e"},"source":["# Customize what happens in Model.fit"]},{"cell_type":"markdown","metadata":{"id":"0a56ffedf331"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/customizing_what_happens_in_fit.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/keras-team/keras-io/blob/master/guides/customizing_what_happens_in_fit.py\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/customizing_what_happens_in_fit.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"7ebb4e65ef9b"},"source":["## Introduction\n","\n","When you're doing supervised learning, you can use `fit()` and everything works\n","smoothly.\n","\n","When you need to write your own training loop from scratch, you can use the\n","`GradientTape` and take control of every little detail.\n","\n","But what if you need a custom training algorithm, but you still want to benefit from\n","the convenient features of `fit()`, such as callbacks, built-in distribution support,\n","or step fusing?\n","\n","A core principle of Keras is **progressive disclosure of complexity**. You should\n","always be able to get into lower-level workflows in a gradual way. You shouldn't fall\n","off a cliff if the high-level functionality doesn't exactly match your use case. You\n","should be able to gain more control over the small details while retaining a\n","commensurate amount of high-level convenience.\n","\n","When you need to customize what `fit()` does, you should **override the training step\n","function of the `Model` class**. This is the function that is called by `fit()` for\n","every batch of data. You will then be able to call `fit()` as usual -- and it will be\n","running your own learning algorithm.\n","\n","Note that this pattern does not prevent you from building models with the Functional\n","API. You can do this whether you're building `Sequential` models, Functional API\n","models, or subclassed models.\n","\n","Let's see how that works."]},{"cell_type":"markdown","source":["지도 학습을 수행할 때 fit()를 사용할 수 있으며 모든 것이 원활하게 작동합니다.\n","\n","훈련 루프를 처음부터 작성해야 하는 경우, GradientTape를 사용하여 모든 세부 사항을 제어할 수 있습니다.\n","\n","그러나 사용자 정의 훈련 알고리즘이 필요하지만 콜백, 내장 배포 지원 또는 단계 융합과 같은 fit()의 편리한 특성을 계속 활용하려면 어떻게 해야 할까요?\n","\n","Keras의 핵심 원칙은 복잡성의 점진적인 공개입니다. 항상 점진적으로 저수준 워크플로부터 시작할 수 있어야 합니다. 높은 수준의 기능이 자신의 사용 사례와 정확하게 일치하지 않다고 해서 절망할 필요는 없습니다. 적절한 수준의 고수준 편의를 유지하면서 작은 세부 사항을 보다 효과적으로 제어할 수 있어야 합니다.\n","\n","fit()를 사용자 정의해야 하는 경우, Model 클래스의 **training step** 함수를 재정의해야 합니다. 이 함수는 모든 데이터 배치에 대해 fit()에 의해 호출되는 함수입니다. 그런 다음 평소와 같이 fit()을 호출 할 수 있으며 자체 학습 알고리즘을 실행합니다.\n","\n","이 패턴은 Functional API를 사용하여 모델을 빌드하는 데 방해가 되지 않습니다. Sequential 모델, Functional API 모델, 또는 하위 클래스화된 모델과 관계없이 수행할 수 있습니다.\n","\n","어떻게 동작하는지 살펴보겠습니다."],"metadata":{"id":"zh5MeWE7QN-L"}},{"cell_type":"markdown","metadata":{"id":"2849e371b9b6"},"source":["## Setup\n","Requires TensorFlow 2.2 or later."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:25.228485Z","iopub.status.busy":"2021-08-25T18:01:25.227886Z","iopub.status.idle":"2021-08-25T18:01:26.623383Z","shell.execute_reply":"2021-08-25T18:01:26.623788Z"},"id":"4dadb6688663","executionInfo":{"status":"ok","timestamp":1665709851908,"user_tz":-540,"elapsed":2908,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","metadata":{"id":"9022333acaa7"},"source":["## A first simple example\n","\n","Let's start from a simple example:\n","\n","- We create a new class that subclasses `keras.Model`.\n","- We just override the method `train_step(self, data)`.\n","- We return a dictionary mapping metric names (including the loss) to their current\n","value.\n","\n","The input argument `data` is what gets passed to fit as training data:\n","\n","- If you pass Numpy arrays, by calling `fit(x, y, ...)`, then `data` will be the tuple\n","`(x, y)`\n","- If you pass a `tf.data.Dataset`, by calling `fit(dataset, ...)`, then `data` will be\n","what gets yielded by `dataset` at each batch.\n","\n","In the body of the `train_step` method, we implement a regular training update,\n","similar to what you are already familiar with. Importantly, **we compute the loss via\n","`self.compiled_loss`**, which wraps the loss(es) function(s) that were passed to\n","`compile()`.\n","\n","Similarly, we call `self.compiled_metrics.update_state(y, y_pred)` to update the state\n","of the metrics that were passed in `compile()`, and we query results from\n","`self.metrics` at the end to retrieve their current value."]},{"cell_type":"markdown","source":["간단한 예제부터 시작하겠습니다.\n","\n","* `keras.Model`을 하위 클래스화하는 새 클래스를 만듭니다.\n","* `train_step(self, data)` 메서드를 재정의합니다.\n","* 손실을 포함하여 dictionary mapping metric names을 현재 값으로 반환합니다.\n","\n","입력 인수 data는 훈련 데이터에 맞게 전달됩니다.\n","\n","* `fit(x, y, ...)`를 호출하여 Numpy 배열을 전달하면 data는 튜플 `(x, y)`가 됩니다.\n","* `tf.data.Dataset`를 전달하는 경우, `fit(dataset, ...)`를 호출하여 data가 각 배치에서 dataset에 의해 산출됩니다.\n","\n","`train_step` 메서드의 본문에서 이미 익숙한 것과 유사한 정기적인 훈련 업데이트를 구현합니다. 중요한 것은 `self.compiled_loss`를 통해 손실을 계산하여 `compile()`로 전달된 손실 함수를 래핑합니다.\n","\n","마찬가지로, `self.compiled_metrics.update_state(y, y_pred)`를 호출하여 `compile()`에 전달된 메트릭의 상태를 업데이트하고, 마지막에 `self.metrics`의 결과를 쿼리하여 현재 값을 검색합니다."],"metadata":{"id":"mqrY5BGqQjxi"}},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:26.630634Z","iopub.status.busy":"2021-08-25T18:01:26.630015Z","iopub.status.idle":"2021-08-25T18:01:26.829818Z","shell.execute_reply":"2021-08-25T18:01:26.830221Z"},"id":"060c8bf4150d","executionInfo":{"status":"ok","timestamp":1665710293166,"user_tz":-540,"elapsed":619,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[],"source":["class CustomModel(keras.Model):\n","    \n","    def train_step(self, data):\n","                \n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        x, y = data\n","        \n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # Forward pass\n","            # Compute the loss value\n","            # (the loss function is configured in `compile()`)\n","            # lapping main loss function\n","            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","\n","        # Compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        # Update metrics (includes the metric that tracks the loss)\n","        self.compiled_metrics.update_state(y, y_pred)\n","        # Return a dict mapping metric names to current value\n","        return {m.name: m.result() for m in self.metrics}\n"]},{"cell_type":"markdown","metadata":{"id":"c9d2cc7a7014"},"source":["Let's try this out:"]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"yLuXuukiSl0q","executionInfo":{"status":"ok","timestamp":1665710296339,"user_tz":-540,"elapsed":466,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:26.836116Z","iopub.status.busy":"2021-08-25T18:01:26.835493Z","iopub.status.idle":"2021-08-25T18:01:29.449984Z","shell.execute_reply":"2021-08-25T18:01:29.450389Z"},"id":"5e6bd7b554f6","executionInfo":{"status":"ok","timestamp":1665710298868,"user_tz":-540,"elapsed":2,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[],"source":["# Construct and compile an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"]},{"cell_type":"code","source":["# Just use `fit` as usual\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.fit(x, y, epochs=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tXN6uFaSu6C","executionInfo":{"status":"ok","timestamp":1665710301421,"user_tz":-540,"elapsed":632,"user":{"displayName":"서성원","userId":"03248215396884205789"}},"outputId":"74e3d359-1528-437f-ae1b-aae6632918ae"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","32/32 [==============================] - 0s 3ms/step - loss: 1.7218 - mae: 1.1931\n","Epoch 2/3\n","32/32 [==============================] - 0s 2ms/step - loss: 0.8083 - mae: 0.7634\n","Epoch 3/3\n","32/32 [==============================] - 0s 2ms/step - loss: 0.4259 - mae: 0.5301\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fadac200690>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"a882cb6467d6"},"source":["## Going lower-level\n","\n","Naturally, you could just skip passing a loss function in `compile()`, and instead do\n","everything *manually* in `train_step`. Likewise for metrics.\n","\n","Here's a lower-level\n","example, that only uses `compile()` to configure the optimizer:\n","\n","- We start by creating `Metric` instances to track our loss and a MAE score.\n","- We implement a custom `train_step()` that updates the state of these metrics\n","(by calling `update_state()` on them), then query them (via `result()`) to return their current average value,\n","to be displayed by the progress bar and to be pass to any callback.\n","- Note that we would need to call `reset_states()` on our metrics between each epoch! Otherwise\n","calling `result()` would return an average since the start of training, whereas we usually work\n","with per-epoch averages. Thankfully, the framework can do that for us: just list any metric\n","you want to reset in the `metrics` property of the model. The model will call `reset_states()`\n","on any object listed here at the beginning of each `fit()` epoch or at the beginning of a call to\n","`evaluate()`."]},{"cell_type":"markdown","source":["당연히 compile()에서 손실 함수의 전달을 건너뛰고, 대신 train_step에서 수동으로 모두 수행할 수 있습니다. 메트릭도 마찬가지입니다.\n","\n","다음은 옵티마이저를 구성하기 위해 compile()만 사용하는 하위 수준의 예입니다.\n","\n","* 먼저 손실과 MAE 점수를 추적하기 위해 Metric 인스턴스를 생성합니다.\n","* (메트릭에 대한 update_state()를 호출하여) 메트릭의 상태를 업데이트하는 사용자 정의 `train_step()`을 구현한 다음, 쿼리하여(result()를 통해) 현재 평균 값을 반환하여 진행률 표시줄에 표시되고 모든 콜백에 전달되도록 합니다.\n","* 각 epoch 사이의 메트릭에 대해 `reset_states()`를 호출해야 합니다. 그렇지 않으면, `result()`를 호출하면 훈련 시작 이후부터 평균이 반환되지만, 일반적으로 epoch당 평균을 사용합니다. 다행히도 프레임워크에서는 다음과 같이 수행할 수 있습니다. 즉, reset하려는 매트릭을 모델의 metrics 속성에 나열하기만 하면 됩니다. 모델은 각 `fit()` epoch가 시작될 때 또는 evaluate() 호출이 시작될 때 여기에 나열된 모든 메트릭 객체에 대해 `reset_states()`를 호출합니다."],"metadata":{"id":"pUh5r8X4Umpa"}},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:29.460415Z","iopub.status.busy":"2021-08-25T18:01:29.459417Z","iopub.status.idle":"2021-08-25T18:01:29.914157Z","shell.execute_reply":"2021-08-25T18:01:29.913719Z"},"id":"2308abf5fe7d","executionInfo":{"status":"ok","timestamp":1665711736407,"user_tz":-540,"elapsed":468,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[],"source":["loss_tracker = keras.metrics.Mean(name=\"loss\")\n","mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n","\n","class CustomModel(keras.Model):\n","    # model.fit() 호출시에 train_step이 호출됨.\n","    # keras.Model의 train_step을 override 함.\n","    def train_step(self, data):\n","        x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # Forward pass\n","            # Compute our own loss\n","            loss = keras.losses.mean_squared_error(y, y_pred)\n","\n","        # Compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Compute our own metrics\n","        loss_tracker.update_state(loss)\n","        mae_metric.update_state(y, y_pred)\n","        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n","\n","    @property   #getter을 만들어준다(여기서는 metrics 함수의 getter)\n","    def metrics(self):\n","        # We list our `Metric` objects here so that `reset_states()` can be\n","        # called automatically at the start of each epoch\n","        # or at the start of `evaluate()`.\n","        # If you don't implement this property, you have to call\n","        # `reset_states()` yourself at the time of your choosing.\n","        return [loss_tracker, mae_metric]"]},{"cell_type":"code","source":["# Construct an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","\n","# We don't passs a loss or metrics here.\n","model.compile(optimizer=\"adam\")\n","\n","# Just use `fit` as usual -- you can use callbacks, etc.\n","x = np.random.random((100000, 32))\n","y = np.random.random((100000, 1))\n","model.fit(x, y, epochs=5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uu9tJp-hVolz","executionInfo":{"status":"ok","timestamp":1665711777801,"user_tz":-540,"elapsed":29322,"user":{"displayName":"서성원","userId":"03248215396884205789"}},"outputId":"e0609ae6-d4c8-49a5-9f37-d641e3f93c1c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","3125/3125 [==============================] - 6s 2ms/step - loss: 0.1703 - mae: 0.3198\n","Epoch 2/5\n","3125/3125 [==============================] - 6s 2ms/step - loss: 0.0841 - mae: 0.2504\n","Epoch 3/5\n","3125/3125 [==============================] - 6s 2ms/step - loss: 0.0837 - mae: 0.2501\n","Epoch 4/5\n","3125/3125 [==============================] - 6s 2ms/step - loss: 0.0834 - mae: 0.2497\n","Epoch 5/5\n","3125/3125 [==============================] - 6s 2ms/step - loss: 0.0833 - mae: 0.2497\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fadac0ad250>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"f451e382c6a8"},"source":["## Supporting `sample_weight` & `class_weight`\n","\n","You may have noticed that our first basic example didn't make any mention of sample\n","weighting. If you want to support the `fit()` arguments `sample_weight` and\n","`class_weight`, you'd simply do the following:\n","\n","- Unpack `sample_weight` from the `data` argument\n","- Pass it to `compiled_loss` & `compiled_metrics` (of course, you could also just apply\n","it manually if you don't rely on `compile()` for losses & metrics)\n","- That's it. That's the list."]},{"cell_type":"markdown","source":["첫 번째 기본 예제에서는 샘플 가중치에 대해 언급하지 않았습니다. fit() 인수 sample_weight 및 class_weight를 지원하려면 다음을 수행하면 됩니다.\n","\n","* data 인수에서 sample_weight 패키지를 언팩합니다\n","* compiled_loss 및 compiled_metrics에 전달합니다(손실 및 메트릭을 위해 compile()에 의존하지 않는다면 수동으로 적용할 수도 있습니다).\n","* 다음은 그 목록입니다."],"metadata":{"id":"vp9I4ysGY3_r"}},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:29.924849Z","iopub.status.busy":"2021-08-25T18:01:29.923826Z","iopub.status.idle":"2021-08-25T18:01:30.579579Z","shell.execute_reply":"2021-08-25T18:01:30.579990Z"},"id":"522d7281f948","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665712064224,"user_tz":-540,"elapsed":1233,"user":{"displayName":"서성원","userId":"03248215396884205789"}},"outputId":"8e85636b-9005-479e-a3a4-9a21362ec7a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","32/32 [==============================] - 0s 3ms/step - loss: 0.2352 - mae: 0.5669\n","Epoch 2/3\n","32/32 [==============================] - 0s 3ms/step - loss: 0.1263 - mae: 0.4030\n","Epoch 3/3\n","32/32 [==============================] - 0s 2ms/step - loss: 0.1167 - mae: 0.3877\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fad927b2210>"]},"metadata":{},"execution_count":12}],"source":["class CustomModel(keras.Model):\n","    def train_step(self, data):\n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        # x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n","        if len(data) == 3:\n","            x, y, sample_weight = data\n","        else:\n","            sample_weight = None\n","            x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # Forward pass\n","            # Compute the loss value.\n","            # The loss function is configured in `compile()`.\n","            loss = self.compiled_loss(\n","                y,\n","                y_pred,\n","                sample_weight=sample_weight,\n","                regularization_losses=self.losses,\n","            )\n","\n","        # Compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update the metrics.\n","        # Metrics are configured in `compile()`.\n","        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n","\n","        # Return a dict mapping metric names to current value.\n","        # Note that it will include the loss (tracked in self.metrics).\n","        return {m.name: m.result() for m in self.metrics}\n","\n","\n","# Construct and compile an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","\n","# You can now use sample_weight argument\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","sw = np.random.random((1000, 1))\n","model.fit(x, y, sample_weight=sw, epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"03000c5590db"},"source":["## Providing your own evaluation step\n","\n","What if you want to do the same for calls to `model.evaluate()`? Then you would\n","override `test_step` in exactly the same way. Here's what it looks like:"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:30.589176Z","iopub.status.busy":"2021-08-25T18:01:30.588394Z","iopub.status.idle":"2021-08-25T18:01:30.792255Z","shell.execute_reply":"2021-08-25T18:01:30.791769Z"},"id":"999edb22c50e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665712190532,"user_tz":-540,"elapsed":473,"user":{"displayName":"서성원","userId":"03248215396884205789"}},"outputId":"2770a268-a744-4927-e87a-4d5a41921072"},"outputs":[{"output_type":"stream","name":"stdout","text":["32/32 [==============================] - 0s 2ms/step - loss: 0.2261 - mae: 0.3860\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.22610382735729218, 0.3859611749649048]"]},"metadata":{},"execution_count":13}],"source":["class CustomModel(keras.Model):\n","    def test_step(self, data):\n","        # Unpack the data\n","        x, y = data\n","        # Compute predictions\n","        y_pred = self(x, training=False)\n","        # Updates the metrics tracking the loss\n","        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","        # Update the metrics.\n","        self.compiled_metrics.update_state(y, y_pred)\n","        # Return a dict mapping metric names to current value.\n","        # Note that it will include the loss (tracked in self.metrics).\n","        return {m.name: m.result() for m in self.metrics}\n","\n","\n","# Construct an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(loss=\"mse\", metrics=[\"mae\"])\n","\n","# Evaluate with our custom test_step\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.evaluate(x, y)"]},{"cell_type":"markdown","metadata":{"id":"9e6a662e6588"},"source":["## Wrapping up: an end-to-end GAN example\n","\n","Let's walk through an end-to-end example that leverages everything you just learned.\n","\n","Let's consider:\n","\n","- A generator network meant to generate 28x28x1 images.\n","- A discriminator network meant to classify 28x28x1 images into two classes (\"fake\" and\n","\"real\").\n","- One optimizer for each.\n","- A loss function to train the discriminator.\n"]},{"cell_type":"markdown","source":["방금 배운 모든 내용을 활용하는 엔드 투 엔드 예제를 살펴보겠습니다.\n","\n","다음을 고려합니다.\n","\n","* generator 네트워크는 28x28x1 이미지를 생성합니다.\n","* discriminator 네트워크는 28x28x1 이미지를 두 개의 클래스(\"false\" 및 \"real\")로 분류하기 위한 것입니다.\n","* 각각 하나의 옵티마이저를 가집니다.\n","* discriminator를 훈련하는 손실 함수."],"metadata":{"id":"hIt98H-qabWK"}},{"cell_type":"markdown","source":["[GAN](https://yamalab.tistory.com/98)"],"metadata":{"id":"gpoj-aJgbWja"}},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:30.804857Z","iopub.status.busy":"2021-08-25T18:01:30.804299Z","iopub.status.idle":"2021-08-25T18:01:30.880921Z","shell.execute_reply":"2021-08-25T18:01:30.881286Z"},"id":"6748db01dc7c","executionInfo":{"status":"ok","timestamp":1665714530442,"user_tz":-540,"elapsed":481,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[],"source":["from tensorflow.keras import layers\n","\n","# Create the discriminator\n","discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(28, 28, 1)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.GlobalMaxPooling2D(),\n","        layers.Dense(1),\n","    ],\n","    name=\"discriminator\",\n",")\n","\n","# Create the generator\n","latent_dim = 128\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n","        layers.Dense(7 * 7 * 128),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Reshape((7, 7, 128)),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n","    ],\n","    name=\"generator\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"801e8dd0c92a"},"source":["Here's a feature-complete GAN class, overriding `compile()` to use its own signature,\n","and implementing the entire GAN algorithm in 17 lines in `train_step`:"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:30.886270Z","iopub.status.busy":"2021-08-25T18:01:30.885713Z","iopub.status.idle":"2021-08-25T18:01:30.893065Z","shell.execute_reply":"2021-08-25T18:01:30.893455Z"},"id":"bc3fb4111393","executionInfo":{"status":"ok","timestamp":1665714622756,"user_tz":-540,"elapsed":663,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[],"source":["class GAN(keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super(GAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        ###\n","        super(GAN, self).compile()\n","        ###\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def train_step(self, real_images): # data -> (x, y, sample_weight)\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","\n","        # Sample random points in the latent space\n","        batch_size = tf.shape(real_images)[0]\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Decode them to fake images\n","        generated_images = self.generator(random_latent_vectors)\n","\n","        # Combine them with real images\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","        # Assemble labels discriminating real from fake images\n","        labels = tf.concat(\n","            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n","        )\n","        # Add random noise to the labels - important trick!\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","\n","        # Train the discriminator\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(combined_images)\n","            d_loss = self.loss_fn(labels, predictions)\n","\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        self.d_optimizer.apply_gradients(\n","            zip(grads, self.discriminator.trainable_weights)\n","        )\n","\n","        # Sample random points in the latent space\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Assemble labels that say \"all real images\"\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        # Train the generator (note that we should *not* update the weights\n","        # of the discriminator)!\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","            \n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"]},{"cell_type":"markdown","metadata":{"id":"095c499a6149"},"source":["Let's test-drive it:"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T18:01:30.899407Z","iopub.status.busy":"2021-08-25T18:01:30.898786Z","iopub.status.idle":"2021-08-25T18:01:35.335627Z","shell.execute_reply":"2021-08-25T18:01:35.335966Z"},"id":"46832f2077ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665714697959,"user_tz":-540,"elapsed":5190,"user":{"displayName":"서성원","userId":"03248215396884205789"}},"outputId":"55b467ce-e71c-411c-aabc-c2cff93e6214"},"outputs":[{"output_type":"stream","name":"stdout","text":["100/100 [==============================] - 4s 31ms/step - d_loss: 0.1942 - g_loss: 1.6473\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fae0008fa10>"]},"metadata":{},"execution_count":17}],"source":["# Prepare the dataset. We use both the training & test MNIST digits.\n","batch_size = 64\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","all_digits = np.concatenate([x_train, x_test])\n","all_digits = all_digits.astype(\"float32\") / 255.0\n","all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n","gan.compile(\n","    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",")\n","\n","# To limit the execution time, we only train on 100 batches. You can train on\n","# the entire dataset. You will need about 20 epochs to get nice results.\n","gan.fit(dataset.take(100), epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"2ed211016c96"},"source":["The ideas behind deep learning are simple, so why should their implementation be painful?"]}],"metadata":{"colab":{"collapsed_sections":[],"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}