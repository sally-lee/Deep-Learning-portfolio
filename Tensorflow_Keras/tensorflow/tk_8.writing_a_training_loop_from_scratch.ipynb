{"cells":[{"cell_type":"markdown","metadata":{"id":"b518b04cbfe0"},"source":["##### Copyright 2020 The TensorFlow Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","execution":{"iopub.execute_input":"2021-08-25T17:46:40.428325Z","iopub.status.busy":"2021-08-25T17:46:40.427703Z","iopub.status.idle":"2021-08-25T17:46:40.431392Z","shell.execute_reply":"2021-08-25T17:46:40.430847Z"},"id":"906e07f6e562"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"daf323e33b84"},"source":["# Writing a training loop from scratch"]},{"cell_type":"markdown","metadata":{"id":"2440f6e0c5ef"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/writing_a_training_loop_from_scratch.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/keras-team/keras-io/blob/master/guides/writing_a_training_loop_from_scratch.py\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/writing_a_training_loop_from_scratch.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"8d4ac441b1fc"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:46:40.439474Z","iopub.status.busy":"2021-08-25T17:46:40.438785Z","iopub.status.idle":"2021-08-25T17:46:42.173951Z","shell.execute_reply":"2021-08-25T17:46:42.174381Z"},"id":"ae2407ad926f"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"0f5a253901f8"},"source":["## Introduction\n","\n","Keras provides default training and evaluation loops, `fit()` and `evaluate()`.\n","Their usage is covered in the guide\n","[Training & evaluation with the built-in methods](https://www.tensorflow.org/guide/keras/train_and_evaluate/).\n","\n","If you want to customize the learning algorithm of your model while still leveraging\n","the convenience of `fit()`\n","(for instance, to train a GAN using `fit()`), you can subclass the `Model` class and\n","implement your own `train_step()` method, which\n","is called repeatedly during `fit()`. This is covered in the guide\n","[Customizing what happens in `fit()`](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/).\n","\n","Now, if you want very low-level control over training & evaluation, you should write\n","your own training & evaluation loops from scratch. This is what this guide is about."]},{"cell_type":"markdown","source":["Keras는 기본 학습 및 평가 루프인 fit() 및 evaluate()를 제공합니다. 사용법은 내장된 메서드를 이용한 훈련 및 평가 가이드에서 다룹니다.\n","\n","fit()의 편리성을 이용하면서 해당 모델의 학습 알고리즘을 커스텀하려면(예를 들어, fit()를 사용하여 GAN을 훈련시킴), Model 클래스를 하위 클래스화하고 fit() 함수 호출 중에 반복적으로 호출되는 고유 train_step() 메서드를 구현할 수 있습니다. 이 내용은 fit() 동작 사용자 정의하기 가이드에서 다룹니다.\n","\n","이제 트레이닝 및 평가에 대한 매우 낮은 수준의 제어를 원한다면 자체 트레이닝 및 평가 루프를 처음부터 작성해야합니다. 이것이이 안내서의 내용입니다."],"metadata":{"id":"W7R7yGhrdVtb"}},{"cell_type":"markdown","metadata":{"id":"f4f47351a3ec"},"source":["## Using the `GradientTape`: a first end-to-end example\n","\n","Calling a model inside a `GradientTape` scope enables you to retrieve the gradients of\n","the trainable weights of the layer with respect to a loss value. Using an optimizer\n","instance, you can use these gradients to update these variables (which you can\n","retrieve using `model.trainable_weights`).\n","\n","Let's consider a simple MNIST model:"]},{"cell_type":"markdown","source":["GradientTape 범위 내에서 모델을 호출하면 손실 값과 관련하여 레이어의 훈련 가능한 가중치 기울기를 가져올 수 있습니다. 최적화 프로그램 인스턴스를 사용하면 이러한 기울기를 이용해 이러한 변수를 업데이트할 수 있습니다(model.trainable_weights를 사용하여 가져올 수 있음).\n","\n","간단한 MNIST 모델을 생각해 보겠습니다."],"metadata":{"id":"uAHxsblWdXyC"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:46:42.179977Z","iopub.status.busy":"2021-08-25T17:46:42.179322Z","iopub.status.idle":"2021-08-25T17:46:43.889759Z","shell.execute_reply":"2021-08-25T17:46:43.890228Z"},"id":"aaa775ce7dab"},"outputs":[],"source":["inputs = keras.Input(shape=(784,), name=\"digits\")\n","x1 = layers.Dense(64, activation=\"relu\")(inputs)\n","x2 = layers.Dense(64, activation=\"relu\")(x1)\n","outputs = layers.Dense(10, name=\"predictions\")(x2)\n","model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"markdown","metadata":{"id":"d8b02a5759cf"},"source":["Let's train it using mini-batch gradient with a custom training loop.\n","\n","First, we're going to need an optimizer, a loss function, and a dataset:"]},{"cell_type":"markdown","source":["사용자 정의 훈련 루프와 함께 미니 배치 기울기를 사용해 이를 훈련시키겠습니다.\n","\n","우선 최적화 프로그램, 손실 함수 및 데이터세트가 필요합니다."],"metadata":{"id":"qSrBTy4odZmO"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:46:43.897935Z","iopub.status.busy":"2021-08-25T17:46:43.897258Z","iopub.status.idle":"2021-08-25T17:46:45.129392Z","shell.execute_reply":"2021-08-25T17:46:45.129878Z"},"id":"f2c6257b8d02","outputId":"24d2e814-34ed-471c-d60d-8eda987ce8f8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665715018638,"user_tz":-540,"elapsed":670,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}],"source":["# Instantiate an optimizer.\n","optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n","# Instantiate a loss function.\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Prepare the training dataset.\n","batch_size = 64\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","x_train = np.reshape(x_train, (-1, 784))\n","x_test = np.reshape(x_test, (-1, 784))\n","\n","# Reserve 10,000 samples for validation.\n","x_val = x_train[-10000:]\n","y_val = y_train[-10000:]\n","x_train = x_train[:-10000]\n","y_train = y_train[:-10000]\n","\n","# Prepare the training dataset.\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","# Prepare the validation dataset.\n","val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n","val_dataset = val_dataset.batch(batch_size)"]},{"cell_type":"markdown","metadata":{"id":"5c30285b1a2e"},"source":["Here's our training loop:\n","\n","- We open a `for` loop that iterates over epochs\n","- For each epoch, we open a `for` loop that iterates over the dataset, in batches\n","- For each batch, we open a `GradientTape()` scope\n","- Inside this scope, we call the model (forward pass) and compute the loss\n","- Outside the scope, we retrieve the gradients of the weights\n","of the model with regard to the loss\n","- Finally, we use the optimizer to update the weights of the model based on the\n","gradients"]},{"cell_type":"markdown","source":["사용자 정의 training loop\n","\n","* 우리는 시대를 반복하는 for 루프를 엽니 다.\n","* 각 시대마다 데이터 집합을 반복적으로 처리하는 for 루프를 엽니 다.\n","* 각 배치마다 GradientTape() 범위를 엽니 다\n","* 이 범위 내에서 모델을 호출하고(순방향 전달) 손실을 계산합니다.\n","* 스코프 외부에서 손실과 관련하여 모델 가중치의 기울기를 검색합니다.\n","* 마지막으로, 그라디언트를 기반으로 모델의 가중치를 업데이트하기 위해 옵티 마이저를 사용합니다"],"metadata":{"id":"kTcZh-f-dcNT"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:46:45.137586Z","iopub.status.busy":"2021-08-25T17:46:45.136897Z","iopub.status.idle":"2021-08-25T17:46:55.198600Z","shell.execute_reply":"2021-08-25T17:46:55.199074Z"},"id":"5bf4c10ceb50","outputId":"d394689d-13ba-4c29-a1ba-d17fde3d4adb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665715461209,"user_tz":-540,"elapsed":14435,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 111.8192\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.9767\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 1.3767\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 1.0698\n","Seen so far: 38464 samples\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 0.9141\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 1.1345\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.6527\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.8066\n","Seen so far: 38464 samples\n"]}],"source":["epochs = 2\n","\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","\n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","\n","        # Open a GradientTape to record the operations run\n","        # during the forward pass, which enables auto-differentiation.\n","        with tf.GradientTape() as tape:\n","\n","            # Run the forward pass of the layer.\n","            # The operations that the layer applies\n","            # to its inputs are going to be recorded\n","            # on the GradientTape.\n","            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n","\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(y_batch_train, logits)\n","\n","        # Use the gradient tape to automatically retrieve\n","        # the gradients of the trainable variables with respect to the loss.\n","        grads = tape.gradient(loss_value, model.trainable_weights)\n","\n","        # Run one step of gradient descent by updating\n","        # the value of the variables to minimize the loss.\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print(\n","                \"Training loss (for one batch) at step %d: %.4f\"\n","                % (step, float(loss_value))\n","            )\n","            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"]},{"cell_type":"markdown","metadata":{"id":"d600076b7be0"},"source":["## Low-level handling of metrics\n","\n","Let's add metrics monitoring to this basic loop.\n","\n","You can readily reuse the built-in metrics (or custom ones you wrote) in such training\n","loops written from scratch. Here's the flow:\n","\n","- Instantiate the metric at the start of the loop\n","- Call `metric.update_state()` after each batch\n","- Call `metric.result()` when you need to display the current value of the metric\n","- Call `metric.reset_states()` when you need to clear the state of the metric\n","(typically at the end of an epoch)\n","\n","Let's use this knowledge to compute `SparseCategoricalAccuracy` on validation data at\n","the end of each epoch:"]},{"cell_type":"markdown","source":["이 기본 루프에 메트릭 모니터링을 추가하겠습니다.\n","\n","처음부터 작성한 이러한 훈련 루프에서 내장 메트릭(또는 사용자가 작성한 사용자 정의 메트릭)을 쉽게 재사용할 수 있습니다. 흐름은 다음과 같습니다.\n","\n","루프가 시작되는 부분에 메트릭 인스턴스화\n","각 배치 끝에서 metric.update_state() 호출\n","메트릭의 현재 값을 표시해야 할 때 metric.result() 호출\n","메트릭의 상태를 지워야 할 때(일반적으로 epoch의 끝에서) metric.reset_states() 호출\n","이러한 지식을 바탕으로 각 epoch가 끝날 때 유효성 검사 데이터에 대해 SparseCategoricalAccuracy를 계산하겠습니다."],"metadata":{"id":"edQanyMddkjy"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:46:55.210546Z","iopub.status.busy":"2021-08-25T17:46:55.209672Z","iopub.status.idle":"2021-08-25T17:46:55.238888Z","shell.execute_reply":"2021-08-25T17:46:55.238327Z"},"id":"2602509b16c7"},"outputs":[],"source":["# Get model\n","inputs = keras.Input(shape=(784,), name=\"digits\")\n","x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n","x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n","outputs = layers.Dense(10, name=\"predictions\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","# Instantiate an optimizer to train the model.\n","optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n","# Instantiate a loss function.\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Prepare the metrics.\n","train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"]},{"cell_type":"markdown","metadata":{"id":"9111a5cc87dc"},"source":["Here's our training & evaluation loop:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:46:55.247898Z","iopub.status.busy":"2021-08-25T17:46:55.247237Z","iopub.status.idle":"2021-08-25T17:47:07.538355Z","shell.execute_reply":"2021-08-25T17:47:07.538834Z"},"id":"654e2311dbff","outputId":"57c28c14-1973-49f3-ba73-eb75958c24ba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665715803916,"user_tz":-540,"elapsed":13344,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 95.2152\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 1.1372\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.9686\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 1.0615\n","Seen so far: 38464 samples\n","Training acc over epoch: 0.7498\n","Validation acc: 0.8439\n","Time taken: 6.65s\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 0.3435\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.3710\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.7966\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.7516\n","Seen so far: 38464 samples\n","Training acc over epoch: 0.8606\n","Validation acc: 0.8862\n","Time taken: 6.62s\n"]}],"source":["import time\n","\n","epochs = 2\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    start_time = time.time()\n","\n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","        with tf.GradientTape() as tape:\n","            logits = model(x_batch_train, training=True)\n","            loss_value = loss_fn(y_batch_train, logits)\n","        grads = tape.gradient(loss_value, model.trainable_weights)\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","        # Update training metric.\n","        train_acc_metric.update_state(y_batch_train, logits)\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print(\n","                \"Training loss (for one batch) at step %d: %.4f\"\n","                % (step, float(loss_value))\n","            )\n","            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n","\n","    # Display metrics at the end of each epoch.\n","    train_acc = train_acc_metric.result()\n","    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","    # Reset training metrics at the end of each epoch\n","    train_acc_metric.reset_states()\n","\n","    # Run a validation loop at the end of each epoch.\n","    for x_batch_val, y_batch_val in val_dataset:\n","        val_logits = model(x_batch_val, training=False)\n","        # Update val metrics\n","        val_acc_metric.update_state(y_batch_val, val_logits)\n","        \n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_states()\n","    print(\"Validation acc: %.4f\" % (float(val_acc),))\n","    print(\"Time taken: %.2fs\" % (time.time() - start_time))"]},{"cell_type":"markdown","metadata":{"id":"1c9a16c21790"},"source":["## Speeding-up your training step with `tf.function`\n","\n","The default runtime in TensorFlow 2 is\n","[eager execution](https://www.tensorflow.org/guide/eager).\n","As such, our training loop above executes eagerly.\n","\n","This is great for debugging, but graph compilation has a definite performance\n","advantage. Describing your computation as a static graph enables the framework\n","to apply global performance optimizations. This is impossible when\n","the framework is constrained to greedly execute one operation after another,\n","with no knowledge of what comes next.\n","\n","You can compile into a static graph any function that takes tensors as input.\n","Just add a `@tf.function` decorator on it, like this:"]},{"cell_type":"markdown","source":["tf.function을 이용해 `train_step`의 속도 높이기\n","\n","TensorFlow 2.0의 기본 런타임은  eager execution입니다. 따라서 위의 훈련 루프는 eager execution 됩니다.\n","\n","이것은 디버깅에는 유용하지만 그래프 컴파일은 확실한 성능 이점이 있습니다. 계산을 정적 그래프로 정의하면 프레임 워크에서 전체 성능 최적화를 적용 할 수 있습니다. 프레임 워크가 다음 작업을 알지 못하고 탐욕스럽게 하나의 작업을 실행하도록 제한되어 있으면 불가능합니다.\n","\n","**텐서를 입력**으로 사용하는 모든 함수를 정적 그래프로 컴파일 할 수 있습니다. 다음과 같이 @tf.function 데코레이터를 추가하십시오."],"metadata":{"id":"dtthk6bsdrTS"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:07.544661Z","iopub.status.busy":"2021-08-25T17:47:07.544004Z","iopub.status.idle":"2021-08-25T17:47:07.545952Z","shell.execute_reply":"2021-08-25T17:47:07.546370Z"},"id":"fdacc2d48ade"},"outputs":[],"source":["@tf.function\n","def train_step(x, y):\n","    with tf.GradientTape() as tape:\n","        logits = model(x, training=True)\n","        loss_value = loss_fn(y, logits)\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","    train_acc_metric.update_state(y, logits)\n","    return loss_value\n"]},{"cell_type":"markdown","metadata":{"id":"ab61b0bf3126"},"source":["Let's do the same with the evaluation step:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:07.550691Z","iopub.status.busy":"2021-08-25T17:47:07.550085Z","iopub.status.idle":"2021-08-25T17:47:07.551990Z","shell.execute_reply":"2021-08-25T17:47:07.552325Z"},"id":"da4828fd8ef7"},"outputs":[],"source":["@tf.function\n","def test_step(x, y):\n","    val_logits = model(x, training=False)\n","    val_acc_metric.update_state(y, val_logits)\n"]},{"cell_type":"markdown","metadata":{"id":"d552377968f1"},"source":["Now, let's re-run our training loop with this compiled training step:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:07.559121Z","iopub.status.busy":"2021-08-25T17:47:07.558493Z","iopub.status.idle":"2021-08-25T17:47:10.428422Z","shell.execute_reply":"2021-08-25T17:47:10.428852Z"},"id":"d69d73c94e44","outputId":"489f612e-b5b1-4af6-e295-bfb91c8d2f9c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665716817374,"user_tz":-540,"elapsed":3937,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 0.1944\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.1237\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.2251\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.6478\n","Seen so far: 38464 samples\n","Training acc over epoch: 0.8866\n","Validation acc: 0.8788\n","Time taken: 1.95s\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 0.2801\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 0.4905\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 0.7773\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 0.6748\n","Seen so far: 38464 samples\n","Training acc over epoch: 0.8982\n","Validation acc: 0.9037\n","Time taken: 1.69s\n"]}],"source":["import time\n","\n","epochs = 2\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    start_time = time.time()\n","\n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","        # train_step running under Graph Mode\n","        loss_value = train_step(x_batch_train, y_batch_train)\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print(\n","                \"Training loss (for one batch) at step %d: %.4f\"\n","                % (step, float(loss_value))\n","            )\n","            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n","\n","    # Display metrics at the end of each epoch.\n","    train_acc = train_acc_metric.result()\n","    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","    # Reset training metrics at the end of each epoch\n","    train_acc_metric.reset_states()\n","\n","    # Run a validation loop at the end of each epoch.\n","    for x_batch_val, y_batch_val in val_dataset:\n","        # test_step running under Graph Mode\n","        test_step(x_batch_val, y_batch_val)\n","\n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_states()\n","    print(\"Validation acc: %.4f\" % (float(val_acc),))\n","    print(\"Time taken: %.2fs\" % (time.time() - start_time))"]},{"cell_type":"markdown","metadata":{"id":"8977d77a8095"},"source":["Much faster, isn't it?"]},{"cell_type":"markdown","metadata":{"id":"b5b5a54d339a"},"source":["## Low-level handling of losses tracked by the model\n","\n","Layers & models recursively track any losses created during the forward pass\n","by layers that call `self.add_loss(value)`. The resulting list of scalar loss\n","values are available via the property `model.losses`\n","at the end of the forward pass.\n","\n","If you want to be using these loss components, you should sum them\n","and add them to the main loss in your training step.\n","\n","Consider this layer, that creates an activity regularization loss:"]},{"cell_type":"markdown","source":["레이어 및 모델은 self.add_loss(value) 를 호출하는 레이어에 의해 순방향 전달 중에 생성되는 모든 손실을 재귀적으로 추적합니다. 스칼라 손실 값의 결과 목록은 순방향 전달이 끝날 때 model.losses 속성을 통해 이용할 수 있습니다.\n","\n","이러한 손실 구성 요소를 사용하려면 해당 구성 요소를 합산하여 training step의 주요 손실에 추가해야합니다.\n","\n","이 계층을 고려하면 활동 정규화 손실이 발생합니다."],"metadata":{"id":"nQqwo6pMeMya"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:10.434532Z","iopub.status.busy":"2021-08-25T17:47:10.433756Z","iopub.status.idle":"2021-08-25T17:47:10.435816Z","shell.execute_reply":"2021-08-25T17:47:10.436239Z"},"id":"4ec7c4b16596"},"outputs":[],"source":["class ActivityRegularizationLayer(layers.Layer):\n","    def call(self, inputs):\n","        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n","        return inputs\n"]},{"cell_type":"markdown","metadata":{"id":"6b12260b8bf2"},"source":["Let's build a really simple model that uses it:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:10.444585Z","iopub.status.busy":"2021-08-25T17:47:10.443445Z","iopub.status.idle":"2021-08-25T17:47:10.492714Z","shell.execute_reply":"2021-08-25T17:47:10.493141Z"},"id":"57afe49e6b93"},"outputs":[],"source":["inputs = keras.Input(shape=(784,), name=\"digits\")\n","x = layers.Dense(64, activation=\"relu\")(inputs)\n","# Insert activity regularization as a layer\n","x = ActivityRegularizationLayer()(x)\n","x = layers.Dense(64, activation=\"relu\")(x)\n","outputs = layers.Dense(10, name=\"predictions\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"markdown","metadata":{"id":"aadb58115c13"},"source":["Here's what our training step should look like now:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:10.499226Z","iopub.status.busy":"2021-08-25T17:47:10.498521Z","iopub.status.idle":"2021-08-25T17:47:10.504239Z","shell.execute_reply":"2021-08-25T17:47:10.503741Z"},"id":"cf674776a0d2"},"outputs":[],"source":["@tf.function\n","def train_step(x, y):\n","    with tf.GradientTape() as tape:\n","        logits = model(x, training=True)\n","        loss_value = loss_fn(y, logits)\n","        # Add any extra losses created during the forward pass.\n","        loss_value += sum(model.losses)\n","        \n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","    train_acc_metric.update_state(y, logits)\n","    return loss_value\n"]},{"cell_type":"code","source":["import time\n","\n","epochs = 2\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    start_time = time.time()\n","\n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","        # train_step running under Graph Mode\n","        loss_value = train_step(x_batch_train, y_batch_train)\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print(\n","                \"Training loss (for one batch) at step %d: %.4f\"\n","                % (step, float(loss_value))\n","            )\n","            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n","\n","    # Display metrics at the end of each epoch.\n","    train_acc = train_acc_metric.result()\n","    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","    # Reset training metrics at the end of each epoch\n","    train_acc_metric.reset_states()\n","\n","    # Run a validation loop at the end of each epoch.\n","    for x_batch_val, y_batch_val in val_dataset:\n","        # test_step running under Graph Mode\n","        test_step(x_batch_val, y_batch_val)\n","\n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_states()\n","    print(\"Validation acc: %.4f\" % (float(val_acc),))\n","    print(\"Time taken: %.2fs\" % (time.time() - start_time))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sHPxScmeK2s","executionInfo":{"status":"ok","timestamp":1665717069494,"user_tz":-540,"elapsed":3396,"user":{"displayName":"서성원","userId":"03248215396884205789"}},"outputId":"d4ee2e59-97fa-4cce-cb73-1f688f6a5700"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Start of epoch 0\n","Training loss (for one batch) at step 0: 2198.4380\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 2.3029\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 2.3024\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 2.3024\n","Seen so far: 38464 samples\n","Training acc over epoch: 0.1117\n","Validation acc: 0.9037\n","Time taken: 1.75s\n","\n","Start of epoch 1\n","Training loss (for one batch) at step 0: 2.3029\n","Seen so far: 64 samples\n","Training loss (for one batch) at step 200: 2.3012\n","Seen so far: 12864 samples\n","Training loss (for one batch) at step 400: 2.3023\n","Seen so far: 25664 samples\n","Training loss (for one batch) at step 600: 2.3033\n","Seen so far: 38464 samples\n","Training acc over epoch: 0.1136\n","Validation acc: 0.9037\n","Time taken: 1.46s\n"]}]},{"cell_type":"markdown","metadata":{"id":"0af04732fe78"},"source":["## Summary\n","\n","Now you know everything there is to know about using built-in training loops and\n","writing your own from scratch.\n","\n","To conclude, here's a simple end-to-end example that ties together everything\n","you've learned in this guide: a DCGAN trained on MNIST digits."]},{"cell_type":"markdown","source":["이제 내장된 훈련 루프를 사용하고 처음부터 고유한 루프를 작성하는 방법에 대해 알아야 할 모든 내용을 살펴보았습니다.\n","\n","결론적으로, 다음은 이 가이드에서 배운 모든 것, 즉 MNIST 숫자에 대해 훈련 된 DCGAN을 하나로 묶는 간단한 종단 간 예제입니다."],"metadata":{"id":"czHmnb3Qehbb"}},{"cell_type":"markdown","metadata":{"id":"9fb325331a1e"},"source":["## End-to-end example: a GAN training loop from scratch\n","\n","You may be familiar with Generative Adversarial Networks (GANs). GANs can generate new\n","images that look almost real, by learning the latent distribution of a training\n","dataset of images (the \"latent space\" of the images).\n","\n","A GAN is made of two parts: a \"generator\" model that maps points in the latent\n","space to points in image space, a \"discriminator\" model, a classifier\n","that can tell the difference between real images (from the training dataset)\n","and fake images (the output of the generator network).\n","\n","A GAN training loop looks like this:\n","\n","1) Train the discriminator.\n","- Sample a batch of random points in the latent space.\n","- Turn the points into fake images via the \"generator\" model.\n","- Get a batch of real images and combine them with the generated images.\n","- Train the \"discriminator\" model to classify generated vs. real images.\n","\n","2) Train the generator.\n","- Sample random points in the latent space.\n","- Turn the points into fake images via the \"generator\" network.\n","- Get a batch of real images and combine them with the generated images.\n","- Train the \"generator\" model to \"fool\" the discriminator and classify the fake images\n","as real.\n","\n","For a much more detailed overview of how GANs works, see\n","[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n","\n","Let's implement this training loop. First, create the discriminator meant to classify\n","fake vs real digits:"]},{"cell_type":"markdown","source":["아마도 GAN(Generative Adversarial Networks)에 익숙할 것으로 생각합니다. GAN은 훈련 데이터세트 이미지의 잠재 분포(이미지의 \"잠재적 공간\")를 학습함으로써 거의 실제처럼 보이는 새로운 이미지를 생성할 수 있습니다.\n","\n","GAN은 잠재적 공간의 포인트를 이미지 공간의 포인트에 매핑하는 \"생성기(generator)\" 모델과 실제 이미지(훈련 데이터세트로 만들어짐)와 가짜 이미지(생성기 네트워크의 출력)의 차이를 구분할 수 있는 분류기인 \"판별자(discriminator)\" 모델의 두 부분으로 이루어집니다.\n","\n","A GAN training loop looks like this:\n","\n","GAN 훈련 루프는 다음과 같습니다.\n","\n","1) discriminator를 훈련시킵니다. \n","* 잠재 공간에서 무작위 포인트 배치를 샘플링합니다. \n","* \"generator\" 모델을 통해 포인트를 가짜 이미지로 변환합니다. \n","* 실제 이미지의 배치를 가져와 생성된 이미지와 결합합니다. \n","* 생성된 이미지와 실제 이미지를 분류하기 위해 \"discriminator\" 모델을 훈련합니다.\n","\n","2) generator를 훈련시키십시오. \n","* 잠재 공간에서 임의의 지점을 샘플링합니다. \n","* \"generator\" 네트워크를 통해 포인트를 가짜 이미지로 전환합니다. \n","* 실제 이미지의 배치를 가져와 생성된 이미지와 결합합니다. \n","* \"generator\" 모델을 훈련시켜 discriminator를 \"속이고\" 가짜 이미지를 진짜로 분류합니다."],"metadata":{"id":"8CK4dWrEetH8"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:10.512549Z","iopub.status.busy":"2021-08-25T17:47:10.511869Z","iopub.status.idle":"2021-08-25T17:47:10.548027Z","shell.execute_reply":"2021-08-25T17:47:10.547541Z"},"id":"fabf9cef3400","outputId":"b90ad8f0-47e4-49a9-f866-5d4bd84d330e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665717940968,"user_tz":-540,"elapsed":310,"user":{"displayName":"서성원","userId":"03248215396884205789"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 14, 14, 64)        640       \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 14, 14, 64)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 7, 7, 128)         73856     \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n","                                                                 \n"," global_max_pooling2d (Globa  (None, 128)              0         \n"," lMaxPooling2D)                                                  \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 74,625\n","Trainable params: 74,625\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(28, 28, 1)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.GlobalMaxPooling2D(),\n","        layers.Dense(1),\n","    ],\n","    name=\"discriminator\",\n",")\n","discriminator.summary()"]},{"cell_type":"markdown","metadata":{"id":"73396eb6daf9"},"source":["Then let's create a generator network,\n","that turns latent vectors into outputs of shape `(28, 28, 1)` (representing\n","MNIST digits):"]},{"cell_type":"markdown","source":["그런 다음, 잠재 벡터를 형상 (28, 28, 1)(MNIST 숫자를 나타냄)의 출력으로 바꾸는 생성기 네트워크를 만듭니다."],"metadata":{"id":"XqBkRDzKfsjS"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:10.560426Z","iopub.status.busy":"2021-08-25T17:47:10.559754Z","iopub.status.idle":"2021-08-25T17:47:10.608540Z","shell.execute_reply":"2021-08-25T17:47:10.608974Z"},"id":"821d203bfb3e"},"outputs":[],"source":["latent_dim = 128\n","\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n","        layers.Dense(7 * 7 * 128),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Reshape((7, 7, 128)),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n","    ],\n","    name=\"generator\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"f0d6d54a78a0"},"source":["Here's the key bit: the training loop. As you can see it is quite straightforward. The\n","training step function only takes 17 lines."]},{"cell_type":"markdown","source":["핵심 비트는 훈련 루프입니다. 보시다시피 매우 간단합니다. 훈련 단계 기능은 17 줄만 사용합니다."],"metadata":{"id":"JXTTYdyMfur6"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:10.618979Z","iopub.status.busy":"2021-08-25T17:47:10.618081Z","iopub.status.idle":"2021-08-25T17:47:10.620821Z","shell.execute_reply":"2021-08-25T17:47:10.620228Z"},"id":"3a11c875142e"},"outputs":[],"source":["# Instantiate one optimizer for the discriminator and another for the generator.\n","d_optimizer = keras.optimizers.Adam(learning_rate=0.0003)\n","g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)\n","\n","# Instantiate a loss function.\n","loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","\n","@tf.function\n","def trains_step(real_images):\n","    # Sample random points in the latent space\n","    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","    # Decode them to fake images\n","    generated_images = generator(random_latent_vectors)\n","    # Combine them with real images\n","    combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","    # Assemble labels discriminating real from fake images\n","    labels = tf.concat(\n","        [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0\n","    )\n","    # Add random noise to the labels - important trick!\n","    labels += 0.05 * tf.random.uniform(labels.shape)\n","\n","    # Train the discriminator\n","    with tf.GradientTape() as tape:\n","        predictions = discriminator(combined_images)\n","        d_loss = loss_fn(labels, predictions)\n","    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n","    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n","\n","    # Sample random points in the latent space\n","    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","    # Assemble labels that say \"all real images\"\n","    misleading_labels = tf.zeros((batch_size, 1))\n","\n","    # Train the generator (note that we should *not* update the weights\n","    # of the discriminator)!\n","    with tf.GradientTape() as tape:\n","        predictions = discriminator(generator(random_latent_vectors))\n","        g_loss = loss_fn(misleading_labels, predictions)\n","    grads = tape.gradient(g_loss, generator.trainable_weights)\n","    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n","    return d_loss, g_loss, generated_images\n"]},{"cell_type":"markdown","metadata":{"id":"fa6bd6292488"},"source":["Let's train our GAN, by repeatedly calling `train_step` on batches of images.\n","\n","Since our discriminator and generator are convnets, you're going to want to\n","run this code on a GPU."]},{"cell_type":"markdown","source":["일련의 이미지에 대해 train_step 을 반복해서 호출하여 GAN을 훈련시켜 봅시다.\n","\n","판별 기 및 생성기는 convnet이므로이 코드를 GPU에서 실행하려고합니다."],"metadata":{"id":"RcyRMjZYfxWL"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-25T17:47:10.628641Z","iopub.status.busy":"2021-08-25T17:47:10.627984Z","iopub.status.idle":"2021-08-25T17:47:21.347540Z","shell.execute_reply":"2021-08-25T17:47:21.347038Z"},"id":"b6a4e3d42262"},"outputs":[],"source":["import os\n","\n","# Prepare the dataset. We use both the training & test MNIST digits.\n","batch_size = 64\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","all_digits = np.concatenate([x_train, x_test]) # all_digits numbers = 70,000 1,094step per epoch\n","all_digits = all_digits.astype(\"float32\") / 255.0\n","all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","epochs = 1  # In practice you need at least 20 epochs to generate nice digits.\n","save_dir = \"./\"\n","\n","for epoch in range(epochs):\n","    print(\"\\nStart epoch\", epoch)\n","\n","    for step, real_images in enumerate(dataset):\n","        print(\"step=\", step)\n","        # Train the discriminator & generator on one batch of real images.\n","        d_loss, g_loss, generated_images = trains_step(real_images)\n","\n","        # Logging.\n","        if step % 200 == 0:\n","            # Print metrics\n","            print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n","            print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))\n","\n","            # Save one generated image\n","            img = tf.keras.preprocessing.image.array_to_img(\n","                generated_images[0] * 255.0, scale=False\n","            )\n","            img.save(os.path.join(save_dir, \"generated_img\" + str(step) + \".png\"))\n","\n","        # To limit execution time we stop after 10 steps.\n","        # Remove the lines below to actually train the model!\n","        # if step > 10:\n","        #     break"]},{"cell_type":"markdown","metadata":{"id":"a92959ac630b"},"source":["That's it! You'll get nice-looking fake MNIST digits after just ~30s of training on the\n","Colab GPU."]},{"cell_type":"code","source":["print(x_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9lsVKp3lLxd","executionInfo":{"status":"ok","timestamp":1665718886358,"user_tz":-540,"elapsed":244,"user":{"displayName":"서성원","userId":"03248215396884205789"}},"outputId":"273454c5-419d-45d8-db0f-3e2cc83bb0e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 28, 28)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2GOEL3mGlRsG"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}